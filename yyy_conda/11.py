import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import warnings
import torch


# æ£€æŸ¥GPUå¯ç”¨æ€§
def check_gpu():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ¯ GPUå¯ç”¨ - ä½¿ç”¨ {torch.cuda.get_device_name(0)} åŠ é€Ÿ")
        return device
    else:
        print("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†å›é€€åˆ°CPU")
        return torch.device("cpu")


# å…¨å±€è®¾å¤‡è®¾ç½®
DEVICE = check_gpu()

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# ================== ä¼˜åŒ–åçš„é…ç½® ==================
subject_predictors = {
    # Natural Sciences
    "physics": {
        "n_clusters": 4,
        "match_papers_L": 100,
        "distance_metric": "euclidean",
        "time_decay_weights": [0.5, 1.0, 1.5],
        "covariance_type": "full",
        "scaling_method": "standard"
    },
    "chemistry": {
        "n_clusters": 3,
        "match_papers_L": 100,
        "distance_metric": "euclidean",
        "covariance_type": "tied",
        "scaling_method": "robust",
        "log_transform": True,
        "dynamic_L": True
    },
    "biology": {
        "n_clusters": 5,
        "match_papers_L": 100,
        "distance_metric": "euclidean",
        "covariance_type": "full",
        "scaling_method": "minmax",
        "dynamic_L": True,
        "use_journal_impact": True
    },

    # Engineering
    "computer_science": {
        "n_clusters": 2,
        "match_papers_L": 100,
        "distance_metric": "manhattan",
        "covariance_type": "diag",
        "scaling_method": "robust",
        "dynamic_L": True
    },
    "mechanical_engineering": {
        "n_clusters": 3,
        "match_papers_L": 100,
        "distance_metric": "euclidean",
        "covariance_type": "tied",
        "scaling_method": "standard",
        "dynamic_L": True,
        "cluster_weighted_mean": True
    },

    # Medicine
    "medical": {
        "n_clusters": 4,
        "match_papers_L": 100,
        "distance_metric": "euclidean",
        "covariance_type": "tied",
        "scaling_method": "robust",
    },

    # Social Sciences
    "economics": {
        "n_clusters": 3,
        "match_papers_L": 100,
        "distance_metric": "cosine",
        "covariance_type": "full",
        "scaling_method": "robust",
        "use_author_hindex": True
    },

    "psychology": {
        "n_clusters": 4,
        "match_papers_L": 100,
        "distance_metric": "cosine",
        "time_decay_weights": [0.8, 1.0, 1.2],
        "covariance_type": "diag",
        "scaling_method": "standard"
    },
    "sociology": {
        "n_clusters": 3,
        "match_papers_L": 100,
        "distance_metric": "cosine",
        "covariance_type": "tied",
        "scaling_method": "minmax",
        "dynamic_L": True,
        "log_transform": True
    },

    # Humanities
    "history": {
        "n_clusters": 2,
        "match_papers_L": 100,
        "distance_metric": "manhattan",
        "covariance_type": "spherical",
        "scaling_method": "minmax",
        "decay_factor": 0.8
    },
    "philosophy": {
        "n_clusters": 2,
        "match_papers_L": 100,
        "distance_metric": "manhattan",
        "covariance_type": "spherical",
        "scaling_method": None,
        "scaling_comment": "standardization disabled"
    }
}

# Chinese to English mapping (unchanged)
chinese_to_english = {
    "ç‰©ç†å­¦": "physics",
    "åŒ–å­¦": "chemistry",
    "ç”Ÿç‰©å­¦": "biology",
    "è®¡ç®—æœºç§‘å­¦": "computer_science",
    "å·¥ç¨‹æŠ€æœ¯": "mechanical_engineering",
    "åŒ»å­¦": "medical",
    "ç»æµå­¦": "economics",
    "å¿ƒç†å­¦": "psychology",
    "ç¤¾ä¼šå­¦": "sociology",
    "å†å²å­¦": "history",
    "å“²å­¦": "philosophy",
}

def get_predictor_config(subject_input):
    """Get predictor configuration by subject name"""
    if subject_input in chinese_to_english:
        english_name = chinese_to_english[subject_input]
        return subject_predictors.get(english_name, None), english_name

    lower_input = subject_input.lower()
    for eng_name in subject_predictors:
        if eng_name.lower() == lower_input:
            return subject_predictors[eng_name], eng_name

    return None, None

class CitationPredictor:
    def __init__(self, train_data, test_data, test_indices, n_train_years=3, m_pred_years=7,
                 n_clusters=3, distance_metric="euclidean", scaling_method="standard", **kwargs):
        """åˆå§‹åŒ–é¢„æµ‹å™¨ï¼ˆæ”¯æŒGPUï¼‰"""
        self.device = DEVICE
        print(f"åˆå§‹åŒ–é¢„æµ‹å™¨ - ä½¿ç”¨è®¾å¤‡: {self.device}")

        # è®­ç»ƒé›†æ•°æ®ï¼ˆè½¬æ¢ä¸ºPyTorchå¼ é‡ï¼‰
        self.train_titles = train_data.iloc[:, 0].values
        self.train_db = torch.tensor(train_data.iloc[:, 13:23].values, device=self.device)

        # æµ‹è¯•é›†æ•°æ®
        self.test_titles = test_data.iloc[:, 0].values
        self.test_db = torch.tensor(test_data.iloc[:, 13:23].values, device=self.device)
        self.test_indices = test_indices

        self.n_train = n_train_years
        self.n_pred = m_pred_years
        self.n_clusters = n_clusters
        self.distance_metric = distance_metric
        self.scaling_method = scaling_method
        self.extra_params = kwargs
        self.use_gpu = kwargs.get('use_gpu', True) and str(self.device) == 'cuda'

        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        if scaling_method == "standard":
            self.scaler = StandardScaler()
        elif scaling_method == "minmax":
            self.scaler = MinMaxScaler()
        elif scaling_method == "robust":
            self.scaler = RobustScaler()
        else:
            self.scaler = None

    def _match_papers(self, test_paper, L=100):
        """GPUåŠ é€Ÿçš„è®ºæ–‡åŒ¹é…"""
        train_X = self.train_db[:, :self.n_train].cpu().numpy()
        test_vec = test_paper[:self.n_train].cpu().numpy()

        if self.use_gpu:
            try:
                from cuml.metrics import pairwise_distances
                if self.distance_metric == "euclidean":
                    distances = pairwise_distances([test_vec], train_X, metric="euclidean")[0]
                elif self.distance_metric == "manhattan":
                    distances = pairwise_distances([test_vec], train_X, metric="cityblock")[0]
                elif self.distance_metric == "cosine":
                    distances = pairwise_distances([test_vec], train_X, metric="cosine")[0]
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {self.distance_metric}")
                return torch.argsort(torch.tensor(distances, device=self.device))[:L].cpu().numpy()
            except ImportError:
                self.use_gpu = False
                print("âš ï¸ RAPIDS cuMLä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUè®¡ç®—")

        # CPUå›é€€æ–¹æ¡ˆ
        if self.distance_metric == "euclidean":
            from sklearn.metrics.pairwise import euclidean_distances
            distances = euclidean_distances([test_vec], train_X)[0]
        elif self.distance_metric == "manhattan":
            from sklearn.metrics.pairwise import manhattan_distances
            distances = manhattan_distances([test_vec], train_X)[0]
        elif self.distance_metric == "cosine":
            from sklearn.metrics.pairwise import cosine_distances
            distances = cosine_distances([test_vec], train_X)[0]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {self.distance_metric}")

        return np.argsort(distances)[:L]

    def _predict_with_gmm(self, matched_indices):
        """GPUåŠ é€Ÿçš„GMMé¢„æµ‹"""
        future_cites = self.train_db[matched_indices, self.n_train:self.n_train + self.n_pred]

        if len(future_cites) < self.n_clusters:
            return np.zeros(self.n_pred)

        # æ•°æ®æ ‡å‡†åŒ–
        if self.scaler:
            scaled_data = self.scaler.fit_transform(future_cites.cpu().numpy())
        else:
            scaled_data = future_cites.cpu().numpy()

        # GMMå»ºæ¨¡
        if self.use_gpu:
            try:
                from cuml import GaussianMixture as cuGaussianMixture
                gmm = cuGaussianMixture(
                    n_components=min(self.n_clusters, len(future_cites)),
                    covariance_type=self.extra_params.get("covariance_type", "full"),
                    max_iter=self.extra_params.get("gmm_max_iter", 200),
                    random_state=42
                )
                gmm.fit(scaled_data)
                means = gmm.means_
            except ImportError:
                self.use_gpu = False
                from sklearn.mixture import GaussianMixture
                gmm = GaussianMixture(
                    n_components=min(self.n_clusters, len(future_cites)),
                    covariance_type=self.extra_params.get("covariance_type", "full"),
                    max_iter=self.extra_params.get("gmm_max_iter", 200),
                    random_state=42
                )
                gmm.fit(scaled_data)
                means = gmm.means_
        else:
            from sklearn.mixture import GaussianMixture
            gmm = GaussianMixture(
                n_components=min(self.n_clusters, len(future_cites)),
                covariance_type=self.extra_params.get("covariance_type", "full"),
                max_iter=self.extra_params.get("gmm_max_iter", 200),
                random_state=42
            )
            gmm.fit(scaled_data)
            means = gmm.means_

        # é€†æ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        if self.scaler:
            pred = self.scaler.inverse_transform(means).mean(axis=0)
        else:
            pred = means.mean(axis=0)

        return np.round(pred).astype(int)

    def predict_test_set(self):
        """é¢„æµ‹æµ‹è¯•é›†æ‰€æœ‰è®ºæ–‡"""
        predictions = []

        for i in range(len(self.test_db)):
            test_paper = self.test_db[i]
            if len(test_paper) < self.n_train + self.n_pred:
                continue

            # åŒ¹é…ç›¸ä¼¼è®ºæ–‡
            matched_idx = self._match_papers(test_paper, L=100)

            if len(matched_idx) == 0:
                continue

            try:
                # GMMé¢„æµ‹
                gmm_pred = self._predict_with_gmm(matched_idx)
                predictions.append({
                    'Paper Title': self.test_titles[i],
                    'True Citations': test_paper[self.n_train:self.n_train + self.n_pred],
                    'Predicted Citations': gmm_pred,
                    'Matched Papers': len(matched_idx),
                    'Original Index': self.test_indices[i]  # å­˜å‚¨åŸå§‹ç´¢å¼•
                })
            except Exception as e:
                continue

        print(f"\næˆåŠŸå¤„ç† {len(predictions)}/{len(self.test_db)} ç¯‡æµ‹è¯•è®ºæ–‡")
        return predictions

def save_predictions(self, predictions, output_path, original_data):
        """ä¿å­˜é¢„æµ‹ç»“æœåˆ°Excel"""
        # åˆ›å»ºåŸå§‹æ•°æ®çš„å‰¯æœ¬
        result_df = original_data.copy()

        # æ·»åŠ æµ‹è¯•é›†æ ‡è®°åˆ—
        if 'Test_Set_Flag' not in result_df.columns:
            result_df['Test_Set_Flag'] = 0

        # æ ‡è®°æµ‹è¯•é›†è®ºæ–‡
        test_indices = [p['Original Index'] for p in predictions]
        result_df.loc[test_indices, 'Test_Set_Flag'] = 1

        # ç§»é™¤å·²æœ‰çš„é¢„æµ‹åˆ—
        for year in range(self.n_pred):
            col_name = f'GMM Pred Year {self.n_train + year + 1}'
            if col_name in result_df.columns:
                result_df.drop(col_name, axis=1, inplace=True)

        # æ’å…¥æ–°çš„é¢„æµ‹åˆ—
        for year in range(self.n_pred):
            col_name = f'GMM Pred Year {self.n_train + year + 1}'
            result_df.insert(23 + year, col_name, np.nan)

        # å¡«å……é¢„æµ‹ç»“æœ
        for pred in predictions:
            idx = pred['Original Index']
            for year_idx in range(self.n_pred):
                col_name = f'GMM Pred Year {self.n_train + year_idx + 1}'
                result_df.at[idx, col_name] = pred['Predicted Citations'][year_idx]

        # ç¡®ä¿æ‰€æœ‰è¦æ±‚çš„åˆ—éƒ½å­˜åœ¨
        expected_columns = [
            'æœŸåˆŠ_ref', 'æœŸåˆŠåˆ†åŒº', 'é¢†åŸŸ', 'å­¦ç§‘', 'æ ‡é¢˜', 'æ ‡é¢˜é•¿åº¦', 'ä½œè€…æ•°é‡',
            'ä½œè€…hæŒ‡æ•°', 'é¡µç ', 'æ€»å¼•ç”¨æ¬¡æ•°', 'å‚è€ƒæ–‡çŒ®æ•°é‡', 'äº”å¹´å½±å“å› å­', 'å‡ºç‰ˆæ—¥æœŸ',
            'year_1', 'year_2', 'year_3', 'year_4', 'year_5', 'year_6',
            'year_7', 'year_8', 'year_9', 'year_10',
            'GMM Pred Year 4', 'GMM Pred Year 5', 'GMM Pred Year 6',
            'GMM Pred Year 7', 'GMM Pred Year 8', 'GMM Pred Year 9',
            'GMM Pred Year 10', 'æ ‡ç­¾', 'èµ·å§‹é¡µç ', 'ç»“æŸé¡µç ', 'ç¯‡å¹…', 'å‡ºç‰ˆç¤¾',
            'åå¹´CNCI', 'Test_Set_Flag'
        ]

        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in expected_columns:
            if col not in result_df.columns:
                result_df[col] = np.nan

        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        result_df = result_df[expected_columns]

        result_df.to_excel(output_path, index=False)
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}")

def process_single_file(input_file, output_file, subject):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        print(f"\nå¤„ç†æ–‡ä»¶: {input_file}")
        print(f"å­¦ç§‘: {subject}")

        # è¯»å–æ•°æ®å¹¶ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•°å­—ç±»å‹
        raw_data = pd.read_excel(input_file)
        for col in raw_data.columns[13:23]:  # å‡è®¾13-22åˆ—æ˜¯å¼•ç”¨æ•°æ®
            raw_data[col] = pd.to_numeric(raw_data[col], errors='coerce').fillna(0)

        # è·å–åŸå§‹ç´¢å¼•
        indices = np.arange(len(raw_data))

        # 9:1 train/test split
        train_data, test_data = train_test_split(
            raw_data,
            test_size=0.1,
            random_state=42
        )
        test_indices = raw_data.index.difference(train_data.index).values

        # åˆå§‹åŒ–é¢„æµ‹å™¨
        predictor = CitationPredictor(
            train_data=train_data,
            test_data=test_data,
            test_indices=test_indices,
            n_train_years=3,
            m_pred_years=7,
            n_clusters=3,
            distance_metric="euclidean",
            scaling_method="standard"
        )

        # è¿›è¡Œé¢„æµ‹
        predictions = predictor.predict_test_set()

        if predictions:
            predictor.save_predictions(predictions, output_file, raw_data)
        else:
            print("æœªç”Ÿæˆæœ‰æ•ˆé¢„æµ‹")

    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")


if __name__ == "__main__":
    # å®‰è£…å¿…è¦çš„GPUåº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if str(DEVICE) == 'cuda':
        print("å»ºè®®å®‰è£…ä»¥ä¸‹GPUåŠ é€Ÿåº“:")
        print("1. PyTorch GPUç‰ˆ: pip install torch torchvision torchaudio")
        print("2. RAPIDS cuML: pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com")

    subjects_to_process = ["ç‰©ç†å­¦", "åŒ–å­¦", "ç”Ÿç‰©å­¦", "è®¡ç®—æœºç§‘å­¦", "å·¥ç¨‹æŠ€æœ¯", "åŒ»å­¦", "ç»æµå­¦", "å¿ƒç†å­¦", "ç¤¾ä¼šå­¦",
                           "å†å²å­¦", "å“²å­¦"]
    print("ğŸš€ å¼€å§‹GPUåŠ é€Ÿæ‰¹é‡å¤„ç†...")

    for subject in subjects_to_process:
        input_file = f"data/{chinese_to_english[subject]}.xlsx"
        output_file = f"gmm/{chinese_to_english[subject]}.xlsx"

        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        process_single_file(input_file, output_file, subject)

    print("\nğŸ‰ GPUåŠ é€Ÿå¤„ç†å®Œæˆ!")